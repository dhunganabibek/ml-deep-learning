{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4eee85",
   "metadata": {},
   "source": [
    "# Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e745e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9971315860748291},\n",
       " {'label': 'NEGATIVE', 'score': 0.9968921542167664}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using sentiment analysis pipeline from Hugging Face Transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    device=0,  # Use GPU if available, otherwise use CPU\n",
    ")\n",
    "classifier([\"I love using Hugging Face Transformers!\", \"I hate waiting in long lines.\"]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b5fc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc1578f402b4dd5afa5e629aec3d90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd5d1f6f89d4c9085cf35fbe5416449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebce23d58dd0431184ad5bc98a4f041d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1010a1cce98e4845905ae6f9a1b64890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5214882dbfda4354bb906493d33eabb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ccc63644584ecda21e6759e593eb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445956110954285, 0.1119767278432846, 0.04342765733599663]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using zero shot classification pipeline from Hugging Face Transformers\n",
    "from transformers import pipeline\n",
    "classifier  = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    device=0,  # Use GPU if available, otherwise use CPU\n",
    ")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a109c592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains.\\n\\nThey were said to have been made up of a single unicorn, which was said to have been able to communicate from its lower jaw to its upper and throat.\\n\\nIt was believed that a single specimen was needed to give researchers a better understanding of the unicorns.\\n\\nThe discovery at the moment has not yet been confirmed by the scientific community.\\n\\nThe team of researchers from the Universidad Nacional de los Andes (UNAM), in Chile, are now working on a project to study the unicorns in a larger area in Bolivia.\\n\\nThe researchers are also hoping to find out more about these creatures.\\n\\nThe team is currently working on a project to study the unicorns in Bolivia. Photo: AFP\\n\\n'We're looking at what the size of these animal was and what the species of unicorn it represents.' said Juan Miguel Vazquez-Puiguas, the project lead from Universidad Nacional de los Andes.\\n\\nThe researchers also plan to study the unicorns in Peru.\\n\\nThe team of scientists will be looking at the size of the unicorns in Peru as well as their evolutionary history.\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generation with Hugging Face Transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "generator  = pipeline(\n",
    "    \"text-generation\",\n",
    "    device=0,  # Use GPU if available, otherwise use CPU\n",
    ")\n",
    "generator(\n",
    "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains.\",\n",
    "    max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff773957",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f3cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "c:\\Users\\Dhung\\source\\repos\\ml-deep-learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2671811878681183,\n",
       "  'token': 277,\n",
       "  'token_str': ' another',\n",
       "  'sequence': 'France is another country.'},\n",
       " {'score': 0.19830884039402008,\n",
       "  'token': 5063,\n",
       "  'token_str': ' neither',\n",
       "  'sequence': 'France is neither country.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using fill masks\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\n",
    "    \"fill-mask\",\n",
    "    device=0,  # Use GPU if available, otherwise use CPU\n",
    ")\n",
    "unmasker(\"France is <mask> country.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af19ac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.5322698950767517, 'start': 0, 'end': 6, 'answer': 'France'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using question answering\n",
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\",\n",
    "    device=0,  # Use GPU if available, otherwise use CPU\n",
    ")\n",
    "question_answerer(\n",
    "    question=\"What is the capital of France?\",\n",
    "    context=\"France is good\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d1de5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70367c67edbf42f683e32cd67c35bae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f68e3255304a46817562dcba51b309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9db038bc014acab0e714d09d485963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec756b6b3d674dd89f603f08fef473f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca770d7d75f44fabbc59d67635a1826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d077bceb11f34abe9e93ded9ba2064aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The Transformers library is an open-source library for natural language processing (NLP) tasks . It provides pre-trained models and tools for tasks such as text classification, question answering, and text generation .'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ussing summarization\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    device=0,  # Use GPU if available, otherwise use CPU\n",
    ")\n",
    "summarizer(\n",
    "    \"The Transformers library is an open-source library for natural language processing (NLP) tasks. It provides pre-trained models and tools for tasks such as text classification, question answering, and text generation. The library is widely used in the NLP community and has become a standard for many applications.\",\n",
    "    max_length=50,\n",
    "    min_length=25,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4539cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:\n",
      " So this is testing how in phrase text to speech feature.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from transformers import pipeline\n",
    "\n",
    "# Parameters\n",
    "fs = 16000  # Sampling rate expected by Whisper\n",
    "seconds = 5  # Duration to record\n",
    "\n",
    "audio = sd.rec(int(seconds * fs), samplerate=fs, channels=1)\n",
    "sd.wait()\n",
    "\n",
    "# Save to a temporary WAV file\n",
    "wav_path = \"mic_input.wav\"\n",
    "write(wav_path, fs, audio)\n",
    "\n",
    "# Load Whisper ASR pipeline\n",
    "transcriber = pipeline(\n",
    "    task=\"automatic-speech-recognition\", model=\"openai/whisper-base.en\"\n",
    ")\n",
    "\n",
    "# Transcribe audio file\n",
    "result = transcriber(wav_path)\n",
    "\n",
    "print(\"Transcription:\")\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e5d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

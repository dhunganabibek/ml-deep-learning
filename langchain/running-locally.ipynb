{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aea2246",
   "metadata": {},
   "source": [
    "# Running Locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see what this query is asking for. The user wants to know the capital of France.\n",
      "\n",
      "First, I need to recall that Paris is indeed the capital city of France. It's a well-known fact and should be straightforward unless there's some trick here. But wait, maybe the user expects something more than just the name? They might want additional information about why it's significant or any historical context around its status as the capital.\n",
      "\n",
      "The system message identifies me as a helpful assistant, so my response should be accurate and concise. The human message is pretty direct: \"What is the capital of France?\" No hidden layers detected here. Just need to answer correctly with Paris. \n",
      "\n",
      "But I should consider if the user might have deeper needs. Are they testing for basic knowledge? Maybe preparing for a test or trivia game? Or perhaps they're looking for confirmation about something they already suspect but want to be sure? Since there's no indication of complexity beyond the obvious, sticking to the direct answer is safe.\n",
      "\n",
      "Also, checking my own knowledge base: France's capital has been Paris since the Middle Ages. No other cities have served as the de facto capital currently or in recent history. So unless the user is asking about a specific time period when Paris wasn't recognized as the capital, which isn't typical here, I should just provide the standard answer.\n",
      "\n",
      "No need to overcomplicate it. The user probably wants a quick and clear response without extra fluff unless they ask for more details. Keeping it simple with \"Paris\" is best.\n",
      "</think>\n",
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "# working with ollama running locally\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# model name to use\n",
    "model_name = \"deepseek-r1:8b\"\n",
    "\n",
    "# question to ask the model\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "# create a chat model chat template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=model_name)\n",
    "\n",
    "# creating a chain to invoke the model with the prompt\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "result = chain.invoke(prompt.format_prompt(question=question).to_messages())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 99.24it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a helpful assistant.\n",
      "\n",
      "User: What is the capital of France?\n",
      "\n",
      "Assistant:\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Choose your DeepSeek model\n",
    "model_name = \"deepseek-ai/deepseek-llm-7b-chat\"\n",
    "\n",
    "# Folder for offloaded weights (if they don't fit in memory)\n",
    "offload_dir = \"./model_offload\"\n",
    "os.makedirs(offload_dir, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with auto device placement + disk offload\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",               # use Accelerate to split across devices\n",
    "    offload_folder=offload_dir,      # store offloaded weights here\n",
    "    trust_remote_code=True           # required for some custom chat templates\n",
    ")\n",
    "\n",
    "# Load generation defaults & fix pad token\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "# Create generation pipeline (no device arg needed with device_map=\"auto\")\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Example chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "# Format chat for the model\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generate\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Output only the assistant's reply\n",
    "reply = result[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
    "print(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1998f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d255c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029acaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29f37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
